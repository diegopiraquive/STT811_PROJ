---
title: "STT Project"
author: "Almalki, Haifa & Piraquive Gomez, Diego "
date: "2025-03-18"
output: word_document
---

This is the complementary notebook code for the STT811 semester project "Predictive Model for Tetanus Vaccination Adherence in the USA" by Almalki, Haifa and Piraquive Gomez, Diego.

All the source txt files can be found in the GitHub Repository: https://github.com/diegopiraquive/STT811_PROJ

All necessary packages to run this notebook are imported in the first cell where they are used.


# Reading the source files
Let’s start by reading the source files, which are in txt format. Since the immunization, conditions, and encounter files contain commas within some fields, this affects their readability. To address this issue, we will apply a custom function to parse each line of these files correctly.

Reading the 'immunizations.txt' file:
```{r}
library(data.table)

# Read all lines from the file
lines_immunization <- readLines("Immunizations.txt")

# Function to parse each line in Immunizations
parse_immunization <- function(line) {
  parts <- strsplit(line, ",", fixed = TRUE)[[1]]
  
  date <- parts[1]
  patient <- parts[2]
  encounter <- parts[3]
  code <- parts[4]
  
  description <- paste(parts[-c(1:4)], collapse = ",")
  
  # Return a data frame
  data.frame(
    DATE = date,
    PATIENT = patient,
    ENCOUNTER = encounter,
    CODE = code,
    DESCRIPTION = description,
    stringsAsFactors = FALSE
  )
}

# Apply function
immunizations <- do.call(rbind, lapply(lines_immunization, parse_immunization))

# Convert to data.table
setDT(immunizations)
```

Reading the 'encounters.txt' file:
```{r}
# Load required library
library(data.table)

# Read all lines from the file
lines_encounters <- readLines("encounters.txt")

# Define a function to parse each line
parse_encounter <- function(line) {
  parts <- strsplit(line, ",", fixed = TRUE)[[1]]
  
  # First 10 fields are fixed; DESCRIPTION may contain commas, so we capture the rest as one field
  if (length(parts) < 10) return(NULL)  # Skip malformed lines
  
  data.frame(
    Id = parts[1],
    START = parts[2],
    STOP = parts[3],
    PATIENT = parts[4],
    ORGANIZATION = parts[5],
    PROVIDER = parts[6],
    PAYER = parts[7],
    ENCOUNTERCLASS = parts[8],
    CODE = parts[9],
    DESCRIPTION = paste(parts[-(1:9)], collapse = ","),  # Remainder as DESCRIPTION
    stringsAsFactors = FALSE
  )
}

# Apply the function to all lines
encounters <- do.call(rbind, lapply(lines_encounters, parse_encounter))

# Convert to data.table
setDT(encounters)
```

Reading the 'patients.txt' file:
```{r}
column_names_patients <- c("Id", "BIRTHDATE", "DEATHDATE", "SSN", "DRIVERS", "PASSPORT", 
                           "PREFIX", "FIRST", "LAST", "SUFFIX", "MAIDEN", "MARITAL", "RACE", 
                           "ETHNICITY", "GENDER", "BIRTHPLACE", "ADDRESS", "CITY", "STATE", 
                           "COUNTY", "FIPS", "ZIP", "LAT", "LON", "HEALTHCARE_EXPENSES", 
                           "HEALTHCARE_COVERAGE", "INCOME", "Mrn")

patients <- fread("Patients.txt", header = FALSE, col.names = column_names_patients, fill = TRUE, na.strings = "\\N")
```

# Initial Data Analysis
Now, let's perform some initial data analysis to check number of rows/columns of our datasets, missing values, data types, duplicates, etc.
```{r}
tables <- list(
  patients = patients,
  encounters = encounters,
  immunizations = immunizations
)

for (name in names(tables)) {
  cat("----", name, "----\n")
  df <- tables[[name]]
  
  # Dimensions
  cat("Rows:", nrow(df), " | Columns:", ncol(df), "\n\n")
  
  # Column types
  cat("Structure:\n")
  print(str(df))
  
  # Summary
  cat("\nSummary:\n")
  print(summary(df))
  
  # Missing values
  cat("\nMissing values per column:\n")
  print(colSums(is.na(df)))
  
  # Duplicates
  cat("\nNumber of duplicate rows:\n")
  print(sum(duplicated(df)))
  
  cat("\n\n")
}

```
Brief summary: We do have some missing values but not in potential predictive features.

Before to continue to an exploratory data analysis, let's perform some feature engineering to get more insights from our data.

Calculating the age of each patient:
```{r}
patients[, BIRTHDATE := as.IDate(BIRTHDATE)]
patients[, DEATHDATE := as.IDate(DEATHDATE)]

patients[, Age := ifelse(is.na(DEATHDATE), 
                         as.integer((as.Date(Sys.Date()) - as.Date(BIRTHDATE)) / 365.25),
                         as.integer((as.Date(DEATHDATE) - as.Date(BIRTHDATE)) / 365.25))]
```

Calculating the number of encounters of each patient and adding it to the patients table. Preliminary we will work with the total number of encounters no matter the type. Further analysis might analyze by type.
```{r}
library(sqldf)
encounter_count <- sqldf("
  SELECT PATIENT, COUNT(*) AS num_encounters
  FROM encounters
  GROUP BY PATIENT
")

# Merge to patients table
patients <- merge(patients, encounter_count, by.x = "Id", by.y = "PATIENT", all.x = TRUE)

# Replace NA with 0
patients$num_encounters[is.na(patients$num_encounters)] <- 0
```

Creating a binary variable identify patients with insurance
```{r}
patients[, Has_Insurance := ifelse(HEALTHCARE_COVERAGE > 0, 1, 0)]
```


# Exploratory Data Analysis
Let's get some insights from the data, starting with looking the most common vaccines.
```{r}

sqldf("SELECT DESCRIPTION, COUNT(*) as Frequency 
              FROM immunizations 
              GROUP BY DESCRIPTION 
              ORDER BY Frequency DESC
              LIMIT 10")
```

We are planning to work with the tetanus vaccine. However, future work might include other vaccines. Let's create a function to easily add an specific vaccine count from the immunizations table to the patients table.

Function to add more vaccines from the immunizations table to the patients table
```{r}
add_vaccine <- function(patients_dt, vaccine_name, new_col_prefix) {
  vaccine_count <- sqldf(paste0("
    SELECT PATIENT, COUNT(*) AS ", new_col_prefix, "_count
    FROM immunizations
    WHERE DESCRIPTION = '", vaccine_name, "'
    GROUP BY PATIENT
  "))

  # Merge step
  patients_dt <- merge(patients_dt, vaccine_count, by.x = "Id", by.y = "PATIENT", all.x = TRUE)

  # Replace NAs with 0
  patients_dt[is.na(get(paste0(new_col_prefix, "_count"))), 
              (paste0(new_col_prefix, "_count")) := 0]

  # Binary indicator
  patients_dt[, (new_col_prefix) := as.integer(get(paste0(new_col_prefix, "_count")) > 0)]

  return(patients_dt)
}
```

Applying the function add_vaccine to add the tetanus count by patient to the patients table.
```{r}
patients <- add_vaccine(patients, 
              "Five doses of tetanus toxoid\\, preservative-free and adsorbed\\, for adults.", 
              "tetanus")
```


Looking at the distribution of the tetatuns vaccine among our patients dataset:
```{r}
sqldf("SELECT tetanus, COUNT(*) AS count
       FROM patients
       GROUP BY tetanus")
```
Seems that our target variable is unbalanced. We will need to handle this imbalance to avoid bias in our predictive model.



Let's get some insights from the features that we might use as a predictors in our classification project.
```{r}
library(ggplot2)

ggplot(patients, aes(x = Age)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Age", x = "Age", y = "Count") +
  theme_minimal()
```

```{r}
ggplot(patients, aes(x = INCOME)) +
  geom_histogram(binwidth = 10000, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Income", x = "Income", y = "Count") +
  theme_minimal()
```

```{r}
ggplot(patients, aes(x = RACE)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Race", x = "Race", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Since most of the patients are white, race does not seems to be a potential predictor.



```{r}
ggplot(patients[HEALTHCARE_COVERAGE < quantile(HEALTHCARE_COVERAGE, 0.95)], 
       aes(x = HEALTHCARE_COVERAGE)) +
  geom_histogram(binwidth = 10000, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Healthcare Coverage (Under 95th Percentile)", 
       x = "Healthcare Coverage", y = "Count") +
  theme_minimal()
```
Some patients shows $0 in insurance coverage, which suggest that they do not have an insurance. That's the reason why we created the binary variable Has_Insurance, to explore to potential predictive power of this variable.

Looking at the insurance coverage distribution:
```{r}
sqldf("
  SELECT 
    Has_Insurance,
    COUNT(*) AS Patient_Count
  FROM patients
  GROUP BY Has_Insurance
")
```

Looking at the gender.
```{r}
ggplot(patients, aes(x = GENDER)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Gender", x = "Gender", y = "Count") +
  theme_minimal()
```
Since we only have one gender, this variable can not be part of our predictors. 

Let's see the linear correlation of some possible predictive features with our target variable tetanus.
```{r, fig.width=8, fig.height=6}
library(ggcorrplot)
corr_vars <- patients[, .(Age, INCOME, Has_Insurance, num_encounters, tetanus)]
corr_vars <- na.omit(corr_vars)

cor_matrix <- cor(corr_vars)

ggcorrplot(cor_matrix, 
           method = "square", 
           lab = TRUE, 
           title = "Correlation Matrix: Vaccination vs Demographics",
           colors = c("red", "white", "blue"))
```


Let's explore further the correlation of age with tetatus:
```{r}
ggplot(patients, aes(x = Age, fill = as.factor(tetanus))) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  scale_fill_manual(values = c("skyblue", "tomato"), name = "Tetanus") +
  labs(title = "Age Distribution by Tetanus Status", x = "Age", y = "Count") +
  theme_minimal()
```

# Pre-processing for Modeling

Let's create a copy of the patients table and re-check for missing values in our interested variables.
```{r}
patients_copy <- patients[, .(Age, Has_Insurance, num_encounters, INCOME, tetanus)]
sapply(patients_copy, function(x) sum(is.na(x)))
patients_copy$tetanus <- as.factor(patients_copy$tetanus)
```
No missing values.

Since the ‘tetanus’ target variable is unbalanced, we are applying SMOTE to balance the dataset by oversampling the minority class.
```{r}
library(ROSE)

set.seed(123) 
tetanus_model <- ovun.sample(tetanus ~ ., data = patients_copy, method = "over", p = 0.5)$data

# Check target balance
table(tetanus_model$tetanus)
```
Now our target variable tetanus looks balanced.

Scaling continuous variables to standardize the data range, improving model training efficiency.
```{r}
tetanus_model$Age <- scale(tetanus_model$Age)
tetanus_model$num_encounters <- scale(tetanus_model$num_encounters)
tetanus_model$INCOME <- scale(tetanus_model$INCOME)
```
Now our data is pre-processed for our models. Let's start with a baseline model: Naive Bayes


# Modeling: Baseline 
Splitting the data:
```{r}
split_pct <- 0.7
n <- nrow(tetanus_model) * split_pct
row_samp <- sample(1:nrow(tetanus_model), n, replace = FALSE)
train_tetanus <- tetanus_model[row_samp, ]
test_tetanus <- tetanus_model[-row_samp, ]
```

Applying Naive Bayes and getting metrics
```{r}
library(e1071)
library(caret)
library(pROC)

tetanus_nb <- naiveBayes(tetanus ~ ., data = train_tetanus)
test_predictions_tetanus_nb <- predict(tetanus_nb, test_tetanus)
train_predictions_tetanus_nb <- predict(tetanus_nb, train_tetanus)

nb_probs_test <- predict(tetanus_nb, test_tetanus, type = "raw")[,"1"]
nb_roc_test <- roc(test_tetanus$tetanus, nb_probs_test)
nb_probs_train <- predict(tetanus_nb, train_tetanus, type = "raw")[,"1"]
nb_roc_train <- roc(train_tetanus$tetanus, nb_probs_train)

# Confusion matrix and accuracy
test_cm_tetanus_nb <- confusionMatrix(test_predictions_tetanus_nb, test_tetanus$tetanus)
train_cm_tetanus_nb <- confusionMatrix(train_predictions_tetanus_nb, train_tetanus$tetanus)
print("Testing Confusion Matrix:")
print(test_cm_tetanus_nb$table)

print("Naïve Bayes Testing Accuracy:")
print(test_cm_tetanus_nb$overall['Accuracy'])

print("Naive Bayes Training Accuracy:")
print(train_cm_tetanus_nb$overall['Accuracy'])
```
We got promising results with our baseline Naive Bayes model, and there are no signs of overfitting, as both the training and testing accuracies are consistent. Now, we’ll explore more advanced machine learning models.

# Modeling
```{r}
library(randomForest)
library(xgboost)

#------------------------------
# XGBoost
#------------------------------

# Preparing data by creating a predictor matrix, converting target to numeric, and building DMatrix objects:
train_x <- model.matrix(tetanus ~ . - 1, data = train_tetanus)
test_x  <- model.matrix(tetanus ~ . - 1, data = test_tetanus)

train_y <- as.numeric(as.character(train_tetanus$tetanus))
test_y  <- as.numeric(as.character(test_tetanus$tetanus))

dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest  <- xgb.DMatrix(data = test_x, label = test_y)

# XGBoost parameters
params <- list(
  objective = "binary:logistic",
  eval_metric = "error",
  eta = 0.1,
  max_depth = 6
)

# XGBoost model for 100 rounds
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = 0
)

# Probabilities
xgb_pred_train_prob <- predict(xgb_model, dtrain)
xgb_pred_test_prob  <- predict(xgb_model, dtest)

# Convert probabilities to factor predictions
xgb_pred_train <- factor(ifelse(xgb_pred_train_prob > 0.5, "1", "0"), levels = c("0", "1"))
xgb_pred_test  <- factor(ifelse(xgb_pred_test_prob > 0.5, "1", "0"), levels = c("0", "1"))

# Compute matrix
xgb_cm_train <- confusionMatrix(xgb_pred_train, train_tetanus$tetanus)
xgb_cm_test  <- confusionMatrix(xgb_pred_test, test_tetanus$tetanus)

xgb_roc_test <- roc(as.numeric(test_tetanus$tetanus) - 1, xgb_pred_test_prob)
xgb_roc_train <- roc(as.numeric(train_tetanus$tetanus) - 1, xgb_pred_train_prob)

#------------------------------
# SVM Model
#------------------------------
svm_model <- svm(tetanus ~ ., data = train_tetanus, kernel = "radial", probability = TRUE)
svm_pred_train <- predict(svm_model, train_tetanus)
svm_pred_test  <- predict(svm_model, test_tetanus)

svm_cm_train <- confusionMatrix(svm_pred_train, train_tetanus$tetanus)
svm_cm_test  <- confusionMatrix(svm_pred_test, test_tetanus$tetanus)

svm_probs_test <- attr(predict(svm_model, test_tetanus, probability = TRUE), "probabilities")[,"1"]
svm_roc_test <- roc(test_tetanus$tetanus, svm_probs_test)
svm_probs_train <- attr(predict(svm_model, train_tetanus, probability = TRUE), "probabilities")[,"1"]
svm_roc_train <- roc(train_tetanus$tetanus, svm_probs_train)

#------------------------------
# Random Forest Model
#------------------------------
rf_model <- randomForest(tetanus ~ ., data = train_tetanus, ntree = 100)
rf_pred_train <- predict(rf_model, train_tetanus)
rf_pred_test  <- predict(rf_model, test_tetanus)

rf_cm_train <- confusionMatrix(rf_pred_train, train_tetanus$tetanus)
rf_cm_test  <- confusionMatrix(rf_pred_test, test_tetanus$tetanus)

rf_probs_test <- predict(rf_model, test_tetanus, type = "prob")[,"1"]
rf_roc_test <- roc(test_tetanus$tetanus, rf_probs_test)
rf_probs_train <- predict(rf_model, train_tetanus, type = "prob")[,"1"]
rf_roc_train <- roc(train_tetanus$tetanus, rf_probs_train)

#------------------------------
# Model Comparison
#------------------------------

# Testing Accuracies:
cat("\nTesting Accuracy Comparison:\n")
cat("Naive Bayes:", round(test_cm_tetanus_nb$overall['Accuracy'] * 100, 2), "%\n")
cat("XGBoost:", round(xgb_cm_test$overall['Accuracy'] * 100, 2), "%\n")
cat("SVM:", round(svm_cm_test$overall['Accuracy'] * 100, 2), "%\n")
cat("Random Forest:", round(rf_cm_test$overall['Accuracy'] * 100, 2), "%\n")

# Training Accuracies:
cat("\nTraining Accuracy Comparison:\n")
cat("Naive Bayes:", round(train_cm_tetanus_nb$overall['Accuracy'] * 100, 2), "%\n")
cat("XGBoost:", round(xgb_cm_train$overall['Accuracy'] * 100, 2), "%\n")
cat("SVM:", round(svm_cm_train$overall['Accuracy'] * 100, 2), "%\n")
cat("Random Forest:", round(rf_cm_train$overall['Accuracy'] * 100, 2), "%\n")
```
Based on this preliminary results, all models demonstrated strong performance, with testing accuracies ranging from 89.69% for Naive Bayes to 97.03% for Random Forest. 

We set standard parameters for our baseline XGBoost model. The ‘binary:logistic’ objective directly matches our goal of categorizing binary outcomes. We used the ‘error’ evaluation metric for a clear measure of performance. A conservative learning rate (Eta) of 0.1 and a maximum depth of 6 were chosen to balance complexity and control overfitting risk. Also, 100 rounds were considered enough for initial learning without excessive computation. 

Training accuracies were similarly high, indicating good model fit. However, to ensure robustness and assess potential overfitting, we will examine additional metrics.

```{r}
library(dplyr)
library(tidyr)
models <- c("Naive Bayes", "XGBoost", "SVM", "Random Forest")

cm_list_test <- list(test_cm_tetanus_nb, xgb_cm_test, svm_cm_test, rf_cm_test)
cm_list_train <- list(train_cm_tetanus_nb, xgb_cm_train, svm_cm_train, rf_cm_train)
roc_list_test <- list(nb_roc_test, xgb_roc_test, svm_roc_test, rf_roc_test)
roc_list_train <- list(nb_roc_train, xgb_roc_train, svm_roc_train, rf_roc_train)

metrics_df_test <- data.frame(
  Model = models,
  Accuracy = sapply(cm_list_test, function(cm) cm$overall['Accuracy']),
  Precision = sapply(cm_list_test, function(cm) cm$byClass["Pos Pred Value"]),
  ROC_AUC = sapply(roc_list_test, function(r) auc(r)),
  F1_Score = sapply(cm_list_test, function(cm) cm$byClass['F1']))

metrics_df_train <- data.frame(
  Model = models,
  Accuracy = sapply(cm_list_train, function(cm) cm$overall['Accuracy']),
  Precision = sapply(cm_list_train, function(cm) cm$byClass["Pos Pred Value"]),
  ROC_AUC = sapply(roc_list_train, function(r) auc(r)),
  F1_Score = sapply(cm_list_train, function(cm) cm$byClass['F1']))

metrics_df_test$Set <- "Test"
metrics_df_train$Set <- "Train"
combined_metrics_df <- bind_rows(metrics_df_test, metrics_df_train)

# Plotting metrics
long_metrics_df <- pivot_longer(combined_metrics_df, cols = c(Accuracy, Precision, ROC_AUC, F1_Score), names_to = "Metric", values_to = "Value")

ggplot(long_metrics_df, aes(x = Model, y = Value, fill = Set)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = sprintf("%.0f%%", Value)), position = position_dodge(width = 0.9), vjust = -0.5, color = "black", size = 3.5) +
  facet_wrap(~ Metric, scales = "free_y") +
  theme_minimal(base_size = 16) + 
  labs(title = "Model Performance Comparison", x = "Model", y = "Metric Value") +
  scale_fill_brewer(palette = "Set1") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
```{r}
print(combined_metrics_df)
```

Based on the previous results:
- Random Forest and XGBoost showed superior precision, suggesting they are better at minimizing false positives.
- XGBoost and Random Forest also exhibit good performance at ROC-AUC, which means they discriminate well between classes.
- All models showed robust F1 scores, indicating a good balance between false positives and false negatives.

The consistency of metrics between the training and testing sets for each model suggests minimal overfitting, indicating that the models are likely to perform well on unseen data, maintaining their predictive quality.

Now extracting the feature importance from the random forest:
```{r}
importance_df <- as.data.frame(importance(rf_model))
importance_df$Feature <- rownames(importance_df)

total_importance <- sum(importance_df$MeanDecreaseGini)
importance_df$Importance_Percent <- (importance_df$MeanDecreaseGini / total_importance) * 100

importance_df <- importance_df[order(-importance_df$Importance_Percent), ]

ggplot(importance_df, aes(x = reorder(Feature, Importance_Percent), y = Importance_Percent)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +  # Flip coordinates to make horizontal bars
  theme_minimal() +
  labs(
    title = "Random Forest Feature Importance",
    x = "Features",
    y = "Relative Importance (%)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
  ) +
  scale_y_continuous(labels = function(y) paste0(round(y, 1), "%")) +
  geom_text(aes(label = paste0(round(Importance_Percent, 1), "%")), 
            hjust = -0.2, size = 3.5)
```

The feature importance plot indicates that Age is the predominant predictor for the tetanus vaccine, contributing to 81% of the model’s decisions. Number of encounters and Income have lesser impacts at 12.2% and 4.5%, respectively, while Has Insurance has minimal influence at 2.2%. This suggests that for the tetanus vaccine, demographic factors like age are crucial, though this might vary with different vaccines.

We can say that our model successfully predicted tetanus vaccination status, with Random Forest and XGBoost demonstrating high accuracy and minimal overfitting. These results can be important for health initiatives to identify unvaccinated individuals. Future steps can include extende analysis to other vaccines, further validation, and practical application within healthcare settings.









